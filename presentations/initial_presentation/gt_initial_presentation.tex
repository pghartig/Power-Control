\documentclass[10pt,tgadventor, onlymath]{beamer}

\usepackage{graphicx,amsmath,amssymb,tikz,psfrag,neuralnetwork}

\input defs.tex
\graphicspath{ {./figures/} }

%% formatting

\mode<presentation>
{
\usetheme{default}
\usecolortheme{seahorse}
}
\setbeamertemplate{navigation symbols}{}
\usecolortheme[rgb={0.03,0.28,0.59}]{structure}
\setbeamertemplate{itemize subitem}{--}
\setbeamertemplate{frametitle} {
	\begin{center}
	  {\large\bf \insertframetitle}
	\end{center}
}

\newcommand\footlineon{
  \setbeamertemplate{footline} {
    \begin{beamercolorbox}[ht=2.5ex,dp=1.125ex,leftskip=.8cm,rightskip=.6cm]{structure}
      \footnotesize \insertsection
      \hfill
      {\insertframenumber}
    \end{beamercolorbox}
    \vskip 0.45cm
  }
}
\footlineon

\AtBeginSection[] 
{ 
	\begin{frame}<beamer> 
		\frametitle{Outline} 
		\tableofcontents[currentsection,currentsubsection] 
	\end{frame} 
} 


\tikzstyle{state}=[shape=circle,draw=blue!30,fill=blue!10]
\tikzstyle{observation}=[shape=rectangle,draw=orange!30,fill=orange!10]
\tikzstyle{lightedge}=[<-, dashed]
\tikzstyle{mainstate}=[state, thick]
\tikzstyle{mainedge}=[<-, thick]
\tikzstyle{block} = [draw,rectangle,thick,minimum height=2em,minimum width=2em]
\tikzstyle{sum} = [draw,circle,inner sep=0mm,minimum size=2mm]
\tikzstyle{connector} = [->,thick]
\tikzstyle{line} = [thick]
\tikzstyle{branch} = [circle,inner sep=0pt,minimum size=1mm,fill=black,draw=black]
\tikzstyle{guide} = []
\tikzstyle{snakeline} = [connector, decorate, decoration={pre length=0.2cm,
                         post length=0.2cm, snake, amplitude=.4mm,
                         segment length=2mm},thick, magenta, ->]



%% begin presentation

\title{\large \bfseries Viterbi Decoding of Complex Channels using Neural Networks}

\author{Peter Hartig\\[3ex]
}

\date{\today}

\begin{document}

\frame{
\thispagestyle{empty}
\titlepage
}

\section{Background}

\begin{frame}
\frametitle{Viterbi Setup}
	Probability of transmitted symbol sequence given the received signal
	\begin{equation}
		Pr(\mathbf{x}|\mathbf{y})
	\end{equation}
	For channels without inter-symbol interference
	\begin{equation}
		\prod_{\mathrm{i=1}}^{\mathrm{n}}pr(x_{\mathrm{i}}|y_{\mathrm{i}}) 
	\end{equation}
	Using Baye's rule
	\begin{equation}
		Pr(x|y) = \frac{Pr(y|x) Pr(x)}{Pr(y)}
	\end{equation}
	With equiprobable transmit symbols
	\begin{equation}
		Pr(y|x) = Pr(x|y) Pr(y)
	\end{equation}

\end{frame}

\begin{frame}
\frametitle{Viterbi Setup Continued}
	Given a finite channel impulse response, and finite symbol constellation, sequence decoding can be formalized as 
	\begin{equation}
	\begin{array}{ll}
		\underset{\mathbf{x}}{\mathrm{maximize}} &\prod_{\mathrm{i=1}}^{\mathrm{N}}Pr(y_{\mathrm{i}}|x_{\mathrm{i}})
		\\ 
		\\
		\underset{\mathbf{x}}{\mathrm{minimize}} &\sum_{\mathrm{i=1}}^{\mathrm{N}} -log(Pr(y_{\mathrm{i}}|x_{\mathrm{i}})) 
	\end{array}
	\end{equation}
	\bigskip
	Example with channel impulse response length 2 and constellation size 2
	\\
	\begin{center}
\begin{tikzpicture}[]
% 1st column
\node               at (-1.5,5) {$s_1=00$};
\node               at (-1.5,4) {$s_2=01$};
\node               at (-1.5,3) {$s_3=10$};
\node               at (-1.5,2) {$s_4=11$};
\node[mainstate] (s1_1) at (0,5) {$s_1$};
\node[state] (s2_1) at (0,4) {$s_2$};
\node[state] (s3_1) at (0,3) {$s_3$};
\node[state] (s4_1) at (0,2) {$s_4$};
%\node at (0,1) {Node1};
% 2nd column
\node[mainstate] (s1_2) at (2,5) {$s_1$}
    edge[mainedge] (s1_1);
\node[mainstate] (s2_2) at (2,4) {$s_2$}
     edge[lightedge] (s1_1);

\node[state] (s3_2) at (2,3) {$s_3$};

\node[state] (s4_2) at (2,2) {$s_4$};

%\node at (2,1) {Node2};
% 3rd column
%\node               at (4,6) {$t=2$};
\node[mainstate] (s1_3) at (4,5) {$s_1$}
    edge[mainedge]  (s1_2);

\node[mainstate] (s2_3) at (4,4) {$s_2$}
    edge[lightedge] (s1_2);

\node[mainstate] (s3_3) at (4,3) {$s_3$}
    edge[mainedge] (s2_2);    
\node[mainstate] (s4_3) at (4,2) {$s_4$}
    edge[lightedge] (s2_2);
%\node at (4,1) {Node3};
% 4th column
%\node               at (6,6) {$t=3$};
\node[mainstate] (s1_4) at (6,5) {$s_1$}
    edge[mainedge]  (s1_3)
    edge[mainedge]  (s3_3);
\node[mainstate] (s2_4) at (6,4) {$s_2$}
    edge[lightedge] (s1_3)
    edge[lightedge] (s3_3);
\node[mainstate] (s3_4) at (6,3) {$s_3$}
    edge[mainedge] (s2_3)
    edge[mainedge] (s4_3);
\node[mainstate] (s4_4) at (6,2) {$s_4$}
    edge[lightedge] (s2_3)
    edge[lightedge] (s4_3);
%\node at (6,1) {Node4};

\end{tikzpicture}
	\end{center}
\end{frame}



\begin{frame}
	\frametitle{Incorporating Neural Net into Viterbi Decoding}
	The metric used to make a decision on the surviving branch of the trellis is based on the metric 
	$Pr(y_{\mathrm{i}}|x_{\mathrm{i}})$. As Neural Networks typically take an observation as the input and output a distribution over a set of classifications, this problem needs to be reformulated.
	For equi-probable transmit symbols, this becomes $Pr(y_{\mathrm{i}}) Pr(x_{\mathrm{i}}|y_{\mathrm{i}})$.
	
\end{frame}


\begin{frame}[fragile]
	\frametitle{Metrics for $Pr(x_{\mathrm{i}}|y_{\mathrm{i}})$}
	\begin{figure}
		\begin{neuralnetwork}[height=4, nodespacing=10mm, layerspacing=15mm]
		\newcommand{\x}[2]{$x_#2$}
		\newcommand{\y}[2]{$\hat{y}_#2$}
		\newcommand{\hfirst}[2]{\small $h^{(1)}_#2$}
		\newcommand{\hsecond}[2]{\small $h^{(2)}_#2$}
		\newcommand{\hthird}[2]{\small $h^{(3)}_#2$}
		\newcommand{\hfourth}[2]{\small $h^{(4)}_#2$}
		\inputlayer[count=1, bias=false, title=Received\\, text=\x]
		\hiddenlayer[count=2, bias=false, title=\\, text=\hthird] \linklayers
		\hiddenlayer[count=3, bias=false, title=\\, text=\hfourth] \linklayers
		\outputlayer[count=4, title=States\\, text=\x] \linklayers
	    \end{neuralnetwork}
	\end{figure}
	%We can take the output of an intermediate layer after training as use it as the input features for a
	%classification neural network.
\end{frame}


\begin{frame}
	\frametitle{Metrics for $r(y_{\mathrm{i}})$}
	Using mixture model from gaussian sources (trained using data used for training Neural Network.
	In particular the Expectation-Maximization is used.
	TODO Discuss problems that I'm having with this
\end{frame}


\begin{frame}
	\frametitle{Full Decoder}
		
\end{frame}


\section{Decoding Environment}

\begin{frame}
\frametitle{Setup}
\end{frame}

\begin{frame}
\frametitle{Viterbi Performance}
	Using 10000 decoded transmit symbols.
	\includegraphics[width=\textwidth]{viterbi_performance}
\end{frame}


\section{Initial Results}

\begin{frame}
\frametitle{Performance with 1000 training examples}
No MM
	\includegraphics[width=\textwidth]{SER_1000symbols}
\end{frame}

\begin{frame}
No MM
\frametitle{Performance with 100 training examples}
	\includegraphics[width=\textwidth]{SER_100symbols}
\end{frame}

\begin{frame}
With MM
\frametitle{Performance with 1000 training examples}
	\includegraphics[width=\textwidth]{SER_1000symbols_mm}
\end{frame}

\end{document}


\begin{frame}
\frametitle{Pictures with tikz}
\begin{center}
\begin{tikzpicture}
	[scale=1.5,dot/.style={circle,draw=black!100,fill=black!100,thick,inner sep=0pt,minimum size=2pt}]
    \node[dot] at (-1,0) (n1) {};
    \node[dot] at (0,1)  (n2) {};
    \node[dot] at (1,0)  (n3) {};
    \node[dot] at (0,-1) (n4) {};
    \draw[gray] (-1.5,0) -- (1.5,0);
    \draw[gray] (0,-1.5) -- (0,1.5);
    \draw[black,thick] (n1) -- (n2) -- (n3) -- (n4) -- (n1) -- cycle;
    \draw[orange,thick] (-1,0.5) -- (0,1) -- (1,1.5);
\end{tikzpicture}
\qquad
\begin{tikzpicture}
	[scale=1.5,dot/.style={circle,draw=black!100,fill=black!100,thick,inner sep=0pt,minimum size=2pt}]
    \draw[gray] (-1.5,0) -- (1.5,0);
    \draw[gray] (0,-1.5) -- (0,1.5);
    \draw[black,thick] (-1,1) -- (0,0) -- (1,1);
\end{tikzpicture}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Pictures with tikz}
\begin{itemize}\itemsep=12pt
	\item convex envelope of (nonconvex) $f$ is the largest convex underestimator $g$
    \item \ie, the best convex lower bound to a function
        \vspace*{1em}
\begin{center}
\begin{tikzpicture}
    \draw[gray] (-1.5,0) -- (1.5,0);
    \draw[gray] (0,-0.5) -- (0,1.5);
    \draw[black,thick] (-1,1) -- (0,0) -- (0.5,0.5) -- (1,-0.25) -- (2,1);
    \draw[black,dashed] (0,0) -- (1,-0.25);
\end{tikzpicture}
\end{center}
    \item \textbf{example}: $\ell_1$ is the envelope of $\card$ (on unit $\ell_\infty$ ball)
    \item \textbf{example}: $\|\cdot\|_*$ is the envelope of $\rank$ (on unit spectral norm ball)
    \item various characterizations: \eg, $f^{**}$ or convex hull of epigraph
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Group lasso \\[-0.3em] 
{\footnotesize \textmd{(\eg, Yuan \& Lin; Meier, van de Geer, B\"uhlmann; Jacob, Obozinski, Vert)}}}
\begin{itemize}\itemsep=12pt
\item problem:
\[
\begin{array}{ll}
\mbox{minimize} & f(x) + \lambda \sum_{i=1}^N \|x_i\|_2
\end{array}
\]
\ie, like lasso, but require groups of variables to be zero or not
\item also called $\ell_{1,2}$ mixed norm regularization
\end{itemize}
\end{frame}

\begin{frame}{Structured group lasso \\[-0.3em] 
{\footnotesize \textmd{(Jacob, Obozinski, Vert; Bach et al.; Zhao, Rocha, Yu; \dots)}}}
\begin{itemize}\itemsep=12pt
\item problem:
\[
\begin{array}{ll}
\mbox{minimize} & f(x) + \sum_{i=1}^N \lambda_i \|x_{g_i}\|_2
\end{array}
\]
where $g_i \subseteq [n]$ and $\mathcal G = \{g_1, \dots, g_N\}$
\item like group lasso, but the groups can overlap arbitrarily
\item particular choices of groups can impose `structured' sparsity
\item \eg, topic models, selecting interaction terms for (graphical) models,
    tree structure of gene networks, fMRI data
\item generalizes to the \textbf{composite absolute penalties family}:
\[
r(x) = \|(\|x_{g_1}\|_{p_1}, \dots, \|x_{g_N}\|_{p_N})\|_{p_0}
\]
\end{itemize}
\end{frame}

\begin{frame}{Structured group lasso \\[-0.3em] 
{\footnotesize \textmd{(Jacob, Obozinski, Vert; Bach et al.; Zhao, Rocha, Yu; \dots)}}}
\textbf{hierarchical selection}:
\begin{center}
\begin{tikzpicture}
[dot/.style={rectangle,draw=black,fill=white,inner sep=5pt,minimum size=5pt}]
\node[dot,draw=orange,thick] at (0,5) (n1) {1};
\node[dot] at (-1,4) (n2) {2};
\node[dot,draw=orange,thick] at (1,4) (n3) {3};
\node[dot] at (-1,3) (n4) {4};
\node[dot,draw=orange,thick] at (0.5,3) (n5) {5};
\node[dot] at (1.5,3) (n6) {6};
\draw[->] (n1) -- (n2);
\draw[->] (n1) -- (n3);
\draw[->] (n2) -- (n4);
\draw[->] (n3) -- (n5);
\draw[->] (n3) -- (n6);
\end{tikzpicture}
\end{center}
\begin{itemize}\itemsep=8pt
    \item $\mathcal G = \{ \{4\}, \textcolor{orange}{\{5\}}, \{6\}, \{2,4\}, 
        \textcolor{orange}{\{3,5,6\}}, \textcolor{orange}{\{1,2,3,4,5,6\} \}}$
\item nonzero variables form a rooted and connected subtree
    \begin{itemize}
        \item if node is selected, so are its ancestors
        \item if node is not selected, neither are its descendants
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Sample ADMM implementation: lasso}
\begin{verbatim}
prox_f = @(v,rho) (rho/(1 + rho))*(v - b) + b;
prox_g = @(v,rho) (max(0, v - 1/rho) - max(0, -v - 1/rho));

AA = A*A';
L  = chol(eye(m) + AA);

for iter = 1:MAX_ITER
    xx = prox_g(xz - xt, rho);
    yx = prox_f(yz - yt, rho);

    yz = L \ (L' \ (A*(xx + xt) + AA*(yx + yt)));
    xz = xx + xt + A'*(yx + yt - yz);
  
    xt = xt + xx - xz;
    yt = yt + yx - yz;
end
\end{verbatim}
\end{frame}

\begin{frame}{Figure}
\begin{center}
\psfrag{k}[t][b]{$k$}
\psfrag{fbest - fmin}[b][t]{$f_\mathrm{best}^{(k)} - f^\star$}
\psfrag{noise-free realize}{noise-free case}
\psfrag{realize1}{realization 1}
\psfrag{realize2}{realization 2}
\includegraphics[height=0.8\textheight]{figures/pwl_error_fbest_realize.eps}
\end{center}
\end{frame}

\begin{frame}{Algorithm}
    if $L$ is not known (usually the case), can use the following line search:
    \noindent\rule[-5pt]{\textwidth}{0.4pt}
    {\footnotesize
    \begin{tabbing}
        {\bf given} $x^k$, $\lambda^{k-1}$, and parameter $\beta \in (0,1)$. \\*[\smallskipamount]
        Let $\lambda := \lambda^{k-1}$. \\*[\smallskipamount]
        {\bf repeat} \\
        \qquad \= 1.\ Let $z := \prox_{\lambda g}(x^{k} - \lambda \nabla f(x^{k}))$. \\
        \> 2.\ {\bf break if} $f(z) \leq \hat{f}_{\lambda}(z, x^{k})$. \\
        \> 3.\ Update $\lambda := \beta \lambda$. \\*[\smallskipamount]
        {\bf return} $\lambda^{k} := \lambda$, $x^{k+1}:=z$.
    \end{tabbing}}
    \noindent\rule[10pt]{\textwidth}{0.4pt}

    typical value of $\beta$ is $1/2$, and 
    \[
    \hat{f}_\lambda(x,y) = f(y) + \nabla f(y)^T (x - y) + 
    (1/2\lambda)\|x - y\|_2^2
    \]
\end{frame}


\end{document}
